{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_1.csv')\n",
    "exog_df_raw = pd.read_csv('Exog_Campaign_eng.csv') # Load raw exog data\n",
    "\n",
    "# Display initial info\n",
    "print('Train Data Info:')\n",
    "train_df.info()\n",
    "print('\\nTrain Data Head:')\n",
    "print(train_df.head())\n",
    "\n",
    "print('\\nExog Data Info:')\n",
    "exog_df_raw.info()\n",
    "print('\\nExog Data Head:')\n",
    "print(exog_df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f577d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exploratory Data Analysis (EDA) --- \n",
    "\n",
    "# Check for null values\n",
    "print('\\nNull values in train_df:')\n",
    "print(train_df.isnull().sum().sort_values(ascending=False))\n",
    "\n",
    "# Calculate percentage of nulls per column (date)\n",
    "null_percentage = train_df.isnull().mean() * 100\n",
    "print('\\nPercentage of null values per date column:')\n",
    "print(null_percentage[null_percentage > 0].sort_values(ascending=False))\n",
    "\n",
    "# Visualize null value distribution over time (optional, can be large)\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# sns.heatmap(train_df.drop('Page', axis=1).isnull(), cbar=False)\n",
    "# plt.title('Null Value Distribution Over Time')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Page Index')\n",
    "# plt.show()\n",
    "\n",
    "# Fill null values with 0, assuming null means no views\n",
    "# This is a common approach, but might need refinement based on domain knowledge\n",
    "train_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the Page column\n",
    "def parse_page_name(page_name):\n",
    "    match = re.match(r'(.*)_([a-z]{2}|[a-z]{2}-.*)\\.wikipedia\\.org_([^_]*)_([^_]*)', page_name)\n",
    "    if match:\n",
    "        title = match.group(1).replace('_', ' ')\n",
    "        language = match.group(2)\n",
    "        access_type = match.group(3)\n",
    "        access_origin = match.group(4)\n",
    "        return pd.Series([title, language, access_type, access_origin])\n",
    "    else:\n",
    "        # Handle cases that don't match the primary pattern (if any)\n",
    "        # Example: Wikimedia commons pages might have a different structure\n",
    "        if 'commons.wikimedia.org' in page_name:\n",
    "             match_commons = re.match(r'(.*)_commons\\.wikimedia\\.org_([^_]*)_([^_]*)', page_name)\n",
    "             if match_commons:\n",
    "                 title = match_commons.group(1).replace('_', ' ')\n",
    "                 language = 'commons' # Assign a specific language code\n",
    "                 access_type = match_commons.group(2)\n",
    "                 access_origin = match_commons.group(3)\n",
    "                 return pd.Series([title, language, access_type, access_origin])\n",
    "        # Fallback if no pattern matches\n",
    "        return pd.Series([page_name, None, None, None])\n",
    "\n",
    "# Apply the function to create new columns\n",
    "parsed_data = train_df['Page'].apply(parse_page_name)\n",
    "parsed_data.columns = ['Title', 'Language', 'AccessType', 'AccessOrigin']\n",
    "\n",
    "# Concatenate the new columns with the original dataframe\n",
    "train_df = pd.concat([train_df, parsed_data], axis=1)\n",
    "\n",
    "# Display value counts for the new columns\n",
    "print('\\nLanguage Distribution:')\n",
    "print(train_df['Language'].value_counts())\n",
    "\n",
    "print('\\nAccess Type Distribution:')\n",
    "print(train_df['AccessType'].value_counts())\n",
    "\n",
    "print('\\nAccess Origin Distribution:')\n",
    "print(train_df['AccessOrigin'].value_counts())\n",
    "\n",
    "print('\\nDataFrame with parsed columns:')\n",
    "print(train_df[['Page', 'Title', 'Language', 'AccessType', 'AccessOrigin']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Visualization --- \n",
    "\n",
    "# Reshape data from wide to long format for easier plotting\n",
    "date_cols = train_df.columns[1:-4] # Select only date columns (exclude Page and parsed cols)\n",
    "train_melted = pd.melt(train_df, \n",
    "                         id_vars=['Page', 'Title', 'Language', 'AccessType', 'AccessOrigin'], \n",
    "                         value_vars=date_cols, \n",
    "                         var_name='Date', \n",
    "                         value_name='Views')\n",
    "\n",
    "# Convert Date column to datetime objects\n",
    "train_melted['Date'] = pd.to_datetime(train_melted['Date'])\n",
    "\n",
    "print('\\nMelted DataFrame Head:')\n",
    "print(train_melted.head())\n",
    "\n",
    "# Calculate total daily views across all pages\n",
    "total_daily_views = train_melted.groupby('Date')['Views'].sum()\n",
    "\n",
    "# Plot total daily views\n",
    "plt.figure(figsize=(18, 6))\n",
    "total_daily_views.plot()\n",
    "plt.title('Total Daily Wikipedia Page Views (All Pages)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Views')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze views by Language\n",
    "avg_views_lang = train_melted.groupby(['Date', 'Language'])['Views'].mean().unstack()\n",
    "\n",
    "# Plot average daily views per language (Top N languages for clarity)\n",
    "top_n = 7 # Adjust as needed\n",
    "top_languages = train_df['Language'].value_counts().nlargest(top_n).index\n",
    "plt.figure(figsize=(18, 6))\n",
    "avg_views_lang[top_languages].plot(ax=plt.gca()) # Plot only top N languages\n",
    "plt.title(f'Average Daily Views per Language (Top {top_n})')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Views')\n",
    "plt.legend(title='Language', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze views by Access Type\n",
    "avg_views_access = train_melted.groupby(['Date', 'AccessType'])['Views'].mean().unstack()\n",
    "plt.figure(figsize=(18, 6))\n",
    "avg_views_access.plot(ax=plt.gca())\n",
    "plt.title('Average Daily Views per Access Type')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Views')\n",
    "plt.legend(title='Access Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze views by Access Origin\n",
    "avg_views_origin = train_melted.groupby(['Date', 'AccessOrigin'])['Views'].mean().unstack()\n",
    "plt.figure(figsize=(18, 6))\n",
    "avg_views_origin.plot(ax=plt.gca())\n",
    "plt.title('Average Daily Views per Access Origin')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Views')\n",
    "plt.legend(title='Access Origin', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stationarity Check --- \n",
    "\n",
    "# We'll use the total_daily_views for initial stationarity checks\n",
    "series_to_check = total_daily_views\n",
    "\n",
    "# Define function for ADF test\n",
    "def adf_test(timeseries):\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    if dftest[1] <= 0.05:\n",
    "        print(\"\\nConclusion: Reject the null hypothesis. Data is likely stationary.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: Fail to reject the null hypothesis. Data is likely non-stationary.\")\n",
    "\n",
    "# Perform ADF test on the original series\n",
    "print(\"--- ADF Test on Original Total Daily Views ---\")\n",
    "adf_test(series_to_check)\n",
    "\n",
    "# Decompose the series \n",
    "# Start with 'additive'. If variance seems to increase with the level in the original plot, try 'multiplicative'.\n",
    "decomposition = seasonal_decompose(series_to_check, model='additive', period=7) # Assuming weekly seasonality (period=7)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(411)\n",
    "plt.plot(series_to_check, label='Original')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.suptitle('Time Series Decomposition (Total Daily Views)')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# Plot ACF and PACF for the original series\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "plot_acf(series_to_check, ax=axes[0], lags=40)\n",
    "plot_pacf(series_to_check, ax=axes[1], lags=40)\n",
    "axes[0].set_title('ACF (Original)')\n",
    "axes[1].set_title('PACF (Original)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccda675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Differencing for Stationarity --- \n",
    "\n",
    "# Apply first-order differencing\n",
    "# We drop NA values created by differencing\n",
    "series_diff1 = series_to_check.diff().dropna()\n",
    "\n",
    "# Perform ADF test on the first-differenced series\n",
    "print(\"\\n--- ADF Test on First-Differenced Series ---\")\n",
    "adf_test(series_diff1)\n",
    "\n",
    "# Plot the differenced series\n",
    "plt.figure(figsize=(18, 6))\n",
    "series_diff1.plot()\n",
    "plt.title('First-Differenced Total Daily Views')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Differenced Views')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot ACF and PACF for the first-differenced series\n",
    "# These plots help determine the p and q parameters for ARIMA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "plot_acf(series_diff1, ax=axes[0], lags=40)\n",
    "plot_pacf(series_diff1, ax=axes[1], lags=40)\n",
    "axes[0].set_title('ACF (1st Difference)')\n",
    "axes[1].set_title('PACF (1st Difference)')\n",
    "plt.show()\n",
    "\n",
    "# Note: If the series is still non-stationary after 1st differencing,\n",
    "# you might need to apply second-order differencing:\n",
    "# series_diff2 = series_diff1.diff().dropna()\n",
    "# adf_test(series_diff2)\n",
    "# Then plot ACF/PACF for series_diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ARIMA Modeling --- \n",
    "\n",
    "# Define train/test split (e.g., forecast last 30 days)\n",
    "forecast_days = 30\n",
    "train_data = series_to_check[:-forecast_days]\n",
    "test_data = series_to_check[-forecast_days:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Define MAPE function\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Avoid division by zero\n",
    "    mask = y_true != 0\n",
    "    # Handle cases where y_true[mask] might still contain zeros if y_pred is non-zero\n",
    "    safe_true = y_true[mask]\n",
    "    safe_pred = y_pred[mask]\n",
    "    non_zero_mask = safe_true != 0\n",
    "    if np.any(non_zero_mask):\n",
    "        return np.mean(np.abs((safe_true[non_zero_mask] - safe_pred[non_zero_mask]) / safe_true[non_zero_mask])) * 100\n",
    "    else:\n",
    "        return 0.0 # Or np.nan, depending on how you want to handle all-zero actuals\n",
    "\n",
    "# Define function for ARIMA modeling using walk-forward validation\n",
    "# Note: Walk-forward is robust but computationally expensive as it refits the model at each step.\n",
    "# For faster (but potentially less accurate) prediction, fit once and predict multiple steps.\n",
    "def run_arima(train, test, order=(7, 1, 7)):\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    \n",
    "    print(f\"Running ARIMA{order} with walk-forward validation...\")\n",
    "    for t in range(len(test)):\n",
    "        try:\n",
    "            model = ARIMA(history, order=order)\n",
    "            model_fit = model.fit() # Refit model at each step\n",
    "            output = model_fit.forecast() # Forecast one step ahead\n",
    "            yhat = output[0]\n",
    "            predictions.append(yhat)\n",
    "            obs = test[t]\n",
    "            history.append(obs) # Add actual observation to history for next prediction\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ARIMA fit/forecast at step {t}: {e}\")\n",
    "            # Handle error, e.g., append last prediction or NaN\n",
    "            predictions.append(predictions[-1] if predictions else np.nan)\n",
    "            history.append(test[t]) # Still add observation\n",
    "            \n",
    "        # Optional: Print progress\n",
    "        if (t+1) % 10 == 0:\n",
    "             print(f'Predicted step {t+1}/{len(test)}')\n",
    "             \n",
    "    # Convert predictions to a pandas Series with the correct index\n",
    "    predictions_series = pd.Series(predictions, index=test.index)\n",
    "    \n",
    "    # Evaluate forecasts\n",
    "    mape = mean_absolute_percentage_error(test, predictions_series.fillna(0)) # Fill potential NaNs\n",
    "    print(f'ARIMA{order} Test MAPE: %.3f' % mape)\n",
    "    \n",
    "    # Plot forecasts against actual outcomes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train.index, train, label='Train')\n",
    "    plt.plot(test.index, test, label='Actual Test')\n",
    "    plt.plot(predictions_series.index, predictions_series, label='ARIMA Forecast', color='red')\n",
    "    plt.title(f'ARIMA{order} Forecast vs Actuals (Total Daily Views)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Views')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return predictions_series, mape\n",
    "\n",
    "# Run ARIMA with initial parameters (p=7, d=1, q=7)\n",
    "# Note: This can be time-consuming.\n",
    "# The order (7,1,7) is just an example based on potential weekly patterns and assumed differencing.\n",
    "# Actual order should be determined from ACF/PACF plots of the differenced series.\n",
    "arima_order = (7, 1, 7) \n",
    "arima_predictions, arima_mape = run_arima(train_data, test_data, arima_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SARIMAX Modeling (with Exogenous Variable) --- \n",
    "\n",
    "# Prepare exogenous data - Assuming exog_df_raw has one column 'Exog'\n",
    "exog_aligned = None # Initialize\n",
    "if 'Exog' in exog_df_raw.columns:\n",
    "    # Check if length matches the main series\n",
    "    if len(exog_df_raw) == len(series_to_check):\n",
    "        # Create a new DataFrame with the correct index and the 'Exog' column\n",
    "        exog_aligned = pd.DataFrame(exog_df_raw['Exog'].values, \n",
    "                                    index=series_to_check.index, \n",
    "                                    columns=['Exog'])\n",
    "        print(\"Exogenous data successfully aligned with main series index.\")\n",
    "    else:\n",
    "        print(f\"Error: Length mismatch between raw exogenous data ({len(exog_df_raw)}) and main series ({len(series_to_check)}). Cannot align.\")\n",
    "else:\n",
    "    print(\"Error: 'Exog' column not found in the exogenous data file.\")\n",
    "\n",
    "# Proceed only if alignment was successful\n",
    "sarimax_predictions, sarimax_mape = None, None # Initialize results\n",
    "if exog_aligned is not None:\n",
    "    # Split exogenous data into train/test\n",
    "    exog_train = exog_aligned[:-forecast_days]\n",
    "    exog_test = exog_aligned[-forecast_days:]\n",
    "\n",
    "    print(f\"Exog Training data shape: {exog_train.shape}\")\n",
    "    print(f\"Exog Testing data shape: {exog_test.shape}\")\n",
    "\n",
    "    # Define function for SARIMAX modeling using walk-forward validation\n",
    "    # Note: Walk-forward is robust but computationally expensive.\n",
    "    def run_sarimax(train, test, exog_train, exog_test, order=(7, 1, 7), seasonal_order=(1, 1, 1, 7)):\n",
    "        history = [x for x in train]\n",
    "        # Ensure exog_history is list of lists/arrays, even for single exog variable\n",
    "        exog_history = exog_train.values.tolist() \n",
    "        predictions = list()\n",
    "        \n",
    "        print(f\"Running SARIMAX{order}{seasonal_order} with walk-forward validation...\")\n",
    "        for t in range(len(test)):\n",
    "            # Ensure current_exog_test is in the correct format (list or array)\n",
    "            current_exog_test = exog_test.iloc[[t]].values \n",
    "            try:\n",
    "                # Note: SARIMAX uses endog and exog covering the same periods for fitting\n",
    "                model = SARIMAX(history, exog=exog_history, order=order, seasonal_order=seasonal_order, \n",
    "                                enforce_stationarity=False, enforce_invertibility=False)\n",
    "                model_fit = model.fit(disp=False) # disp=False suppresses convergence messages; Refit model at each step\n",
    "                \n",
    "                # Forecast one step ahead using the corresponding future exogenous variable\n",
    "                output = model_fit.forecast(steps=1, exog=current_exog_test)\n",
    "                yhat = output[0]\n",
    "                predictions.append(yhat)\n",
    "                \n",
    "                # Update history with actual observation and its corresponding exog variable\n",
    "                obs = test[t]\n",
    "                history.append(obs)\n",
    "                exog_history.append(current_exog_test[0].tolist()) # Append the list/array for the current step\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during SARIMAX fit/forecast at step {t}: {e}\")\n",
    "                predictions.append(predictions[-1] if predictions else np.nan)\n",
    "                # Still update history even if forecast failed\n",
    "                history.append(test[t]) \n",
    "                exog_history.append(current_exog_test[0].tolist())\n",
    "\n",
    "            # Optional: Print progress\n",
    "            if (t+1) % 10 == 0:\n",
    "                 print(f'Predicted step {t+1}/{len(test)}')\n",
    "                 \n",
    "        # Convert predictions to a pandas Series with the correct index\n",
    "        predictions_series = pd.Series(predictions, index=test.index)\n",
    "        \n",
    "        # Evaluate forecasts\n",
    "        mape = mean_absolute_percentage_error(test, predictions_series.fillna(0))\n",
    "        print(f'SARIMAX{order}{seasonal_order} Test MAPE: %.3f' % mape)\n",
    "        \n",
    "        # Plot forecasts against actual outcomes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(train.index, train, label='Train')\n",
    "        plt.plot(test.index, test, label='Actual Test')\n",
    "        plt.plot(predictions_series.index, predictions_series, label='SARIMAX Forecast', color='green')\n",
    "        plt.title(f'SARIMAX{order}{seasonal_order} Forecast vs Actuals (Total Daily Views)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Views')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        return predictions_series, mape\n",
    "\n",
    "    # Run SARIMAX with initial parameters\n",
    "    # Note: This can be even more time-consuming than ARIMA.\n",
    "    # Orders are examples; seasonal_order=(1,1,1,7) assumes weekly seasonality.\n",
    "    sarimax_order = arima_order # Use the same non-seasonal order from ARIMA example\n",
    "    sarimax_seasonal_order = (1, 1, 1, 7) \n",
    "    sarimax_predictions, sarimax_mape = run_sarimax(train_data, test_data, exog_train, exog_test, \n",
    "                                                    order=sarimax_order, seasonal_order=sarimax_seasonal_order)\n",
    "\n",
    "    # Important Note: The exogenous data provided is only for English pages.\n",
    "    # Applying it directly to the 'total_daily_views' (which includes all languages) \n",
    "    # might not be accurate. Ideally, SARIMAX with exog should be run specifically \n",
    "    # on the aggregated views for English pages only, or on individual English page series.\n",
    "    # For this example, we proceed with total views, but acknowledge this limitation.\n",
    "else:\n",
    "    print(\"\\nSkipping SARIMAX modeling due to exogenous data alignment issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033eb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Facebook Prophet Modeling --- \n",
    "\n",
    "# Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
    "prophet_train_df = train_data.reset_index()\n",
    "prophet_train_df.columns = ['ds', 'y']\n",
    "\n",
    "# Instantiate and fit the Prophet model\n",
    "# Prophet automatically detects yearly and weekly seasonality by default\n",
    "print(\"\\nRunning Prophet model...\")\n",
    "model_prophet = Prophet()\n",
    "\n",
    "# Add campaign dates as holidays/events if desired (especially if running on English data)\n",
    "# Check if exog_aligned exists and is not None before using it\n",
    "# if exog_aligned is not None:\n",
    "#     campaign_dates = exog_aligned[exog_aligned['Exog'] == 1].index\n",
    "#     if not campaign_dates.empty:\n",
    "#         holidays = pd.DataFrame({\n",
    "#           'holiday': 'campaign',\n",
    "#           'ds': campaign_dates,\n",
    "#           'lower_window': 0,\n",
    "#           'upper_window': 1, # Assume effect lasts for a day or two\n",
    "#         })\n",
    "#         print(\"Adding campaign dates as holidays to Prophet model.\")\n",
    "#         model_prophet = Prophet(holidays=holidays)\n",
    "#     else:\n",
    "#         print(\"No campaign dates found in exogenous data for Prophet holidays.\")\n",
    "# else:\n",
    "#     print(\"Exogenous data not available for Prophet holidays.\")\n",
    "\n",
    "model_prophet.fit(prophet_train_df)\n",
    "\n",
    "# Create a future dataframe for the forecast period\n",
    "future = model_prophet.make_future_dataframe(periods=forecast_days)\n",
    "\n",
    "# Generate predictions\n",
    "forecast = model_prophet.predict(future)\n",
    "\n",
    "# Extract the forecast for the test period\n",
    "prophet_predictions = forecast.set_index('ds')['yhat'][-forecast_days:]\n",
    "\n",
    "# Evaluate forecasts\n",
    "prophet_mape = mean_absolute_percentage_error(test_data, prophet_predictions)\n",
    "print(f'Prophet Test MAPE: %.3f' % prophet_mape)\n",
    "\n",
    "# Plot forecasts against actual outcomes\n",
    "fig1 = model_prophet.plot(forecast)\n",
    "plt.title('Prophet Forecast (Total Daily Views)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Views')\n",
    "# Add actual test data points to the plot\n",
    "plt.plot(test_data.index, test_data, '.r', label='Actual Test Data') \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Prophet components (trend, weekly, yearly seasonality)\n",
    "fig2 = model_prophet.plot_components(forecast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ARIMA Parameter Tuning (Grid Search Example) --- \n",
    "\n",
    "# Define a function to evaluate an ARIMA model using walk-forward validation\n",
    "# Note: This is computationally expensive, especially within a grid search.\n",
    "def evaluate_arima_model(train, test, order):\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        try:\n",
    "            model = ARIMA(history, order=order)\n",
    "            model_fit = model.fit() # Refit model at each step\n",
    "            yhat = model_fit.forecast()[0]\n",
    "            predictions.append(yhat)\n",
    "            history.append(test[t])\n",
    "        except Exception as e:\n",
    "            # Suppress errors during grid search for cleaner output, return high MAPE\n",
    "            # print(f\"Error evaluating {order} at step {t}: {e}\") \n",
    "            return float('inf') # Return infinity for errors\n",
    "    \n",
    "    predictions_series = pd.Series(predictions, index=test.index)\n",
    "    mape = mean_absolute_percentage_error(test, predictions_series.fillna(0))\n",
    "    return mape\n",
    "\n",
    "# Define a function to perform grid search\n",
    "def grid_search_arima(train, test, p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    total_configs = len(p_values) * len(d_values) * len(q_values)\n",
    "    count = 0\n",
    "    print(f\"Starting ARIMA Grid Search ({total_configs} configurations). This may take a long time...\")\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                count += 1\n",
    "                try:\n",
    "                    mape = evaluate_arima_model(train, test, order)\n",
    "                    if mape < best_score:\n",
    "                        best_score, best_cfg = mape, order\n",
    "                    print(f'[{count}/{total_configs}] ARIMA{order} MAPE=%.3f (Best: {best_cfg} MAPE=%.3f)' % (mape, best_score))\n",
    "                except Exception as e:\n",
    "                    # This outer try-except might catch errors not handled in evaluate_arima_model\n",
    "                    print(f'[{count}/{total_configs}] ARIMA{order} failed unexpectedly: {e}')\n",
    "                    continue\n",
    "    print('\\nGrid Search Complete.')\n",
    "    print('Best ARIMA%s MAPE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg, best_score\n",
    "\n",
    "# Define parameter ranges (keep small for demonstration)\n",
    "# A more thorough search would use wider ranges and potentially tools like pmdarima.auto_arima\n",
    "p_values = [0, 1, 2, 7] # Include 7 based on potential weekly pattern\n",
    "d_values = [1] # Assuming first differencing worked based on earlier ADF test\n",
    "q_values = [0, 1, 2, 7]\n",
    "\n",
    "# --- Run Grid Search (Commented Out By Default) --- \n",
    "# Uncomment the following lines to perform the grid search.\n",
    "# WARNING: This will be very time-consuming due to walk-forward validation inside the loop.\n",
    "# Consider reducing p/q ranges or using a faster evaluation method for initial exploration.\n",
    "# best_order, best_mape = grid_search_arima(train_data, test_data, p_values, d_values, q_values)\n",
    "# if best_order:\n",
    "#    print(f\"\\nBest ARIMA order found: {best_order} with MAPE: {best_mape:.3f}\")\n",
    "#    # Re-run ARIMA with potentially better parameters found from grid search\n",
    "#    print(\"\\nRe-running ARIMA with best found parameters...\")\n",
    "#    arima_predictions_best, arima_mape_best = run_arima(train_data, test_data, best_order)\n",
    "# else:\n",
    "#    print(\"\\nGrid search did not complete successfully or was skipped.\")\n",
    "\n",
    "print(\"\\nNote: Grid search code is present but commented out by default due to high computational cost.\")\n",
    "print(\"Uncomment the relevant lines above to perform the search.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline for Multiple Series (Example) --- \n",
    "\n",
    "# Calculate aggregated daily views per language\n",
    "lang_daily_views = train_melted.groupby(['Date', 'Language'])['Views'].sum().unstack()\n",
    "\n",
    "# Fill potential NaNs that might appear after unstacking if a language has no pages on a day\n",
    "lang_daily_views.fillna(0, inplace=True)\n",
    "\n",
    "print(\"\\nAggregated Daily Views per Language (Head):\")\n",
    "print(lang_daily_views.head())\n",
    "\n",
    "# Define a pipeline function (using ARIMA as an example)\n",
    "# Note: This uses the computationally expensive walk-forward validation from run_arima.\n",
    "# For many series, consider fitting once and forecasting, or using faster models like Prophet/ML.\n",
    "def forecast_pipeline(series, series_name, order=(7, 1, 7), forecast_days=30):\n",
    "    print(f\"\\n--- Running Forecast Pipeline for: {series_name} ---\")\n",
    "    \n",
    "    # Basic check for sufficient data\n",
    "    if len(series) < forecast_days + 50: # Need enough data for training & model stability\n",
    "        print(f\"Skipping {series_name}: Insufficient data points ({len(series)}).\")\n",
    "        return None, None\n",
    "        \n",
    "    # Optional: Add stationarity check and dynamic order selection (e.g., auto_arima) here for a robust pipeline\n",
    "    # try:\n",
    "    #     adf_result = adfuller(series)\n",
    "    #     if adf_result[1] > 0.05: # If not stationary\n",
    "    #         # Apply differencing or use auto_arima to find d\n",
    "    #         pass \n",
    "    # except Exception as e:\n",
    "    #     print(f\"ADF test failed for {series_name}: {e}\")\n",
    "    #     # Handle failure - maybe skip or use default differencing\n",
    "\n",
    "    # Split data\n",
    "    train = series[:-forecast_days]\n",
    "    test = series[-forecast_days:]\n",
    "    \n",
    "    # Run ARIMA (using the previously defined function)\n",
    "    # In a production pipeline, you might:\n",
    "    # 1. Use auto_arima to find the best order for *this specific series*.\n",
    "    # 2. Choose the model (ARIMA, SARIMAX, Prophet) dynamically based on series characteristics.\n",
    "    # 3. Handle exogenous variables appropriately if using SARIMAX (requires matching exog data).\n",
    "    try:\n",
    "        # Using the pre-defined run_arima which includes plotting (remove plot in production)\n",
    "        predictions, mape = run_arima(train, test, order)\n",
    "        print(f\"Finished pipeline for {series_name}. MAPE: {mape:.3f}\")\n",
    "        return predictions, mape\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ARIMA pipeline for {series_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Apply Pipeline to Selected Languages --- \n",
    "\n",
    "languages_to_forecast = ['en', 'ja'] # Example languages\n",
    "results = {}\n",
    "\n",
    "for lang in languages_to_forecast:\n",
    "    if lang in lang_daily_views.columns:\n",
    "        series = lang_daily_views[lang]\n",
    "        # Use the default ARIMA order for this example pipeline.\n",
    "        # Ideally, determine order per series (e.g., using auto_arima or grid search within pipeline)\n",
    "        # or use a model like Prophet that doesn't require order selection.\n",
    "        # The order (7,1,7) might not be optimal for 'en' or 'ja'.\n",
    "        predictions, mape = forecast_pipeline(series, f\"Language: {lang}\", order=arima_order, forecast_days=forecast_days)\n",
    "        results[lang] = {'predictions': predictions, 'mape': mape}\n",
    "    else:\n",
    "        print(f\"\\nLanguage '{lang}' not found in columns.\")\n",
    "\n",
    "# Display overall results\n",
    "print(\"\\n--- Pipeline Summary --- \")\n",
    "for lang, result in results.items():\n",
    "    if result.get('mape') is not None:\n",
    "        print(f\"Language: {lang}, MAPE: {result['mape']:.3f}\")\n",
    "    else:\n",
    "        print(f\"Language: {lang}, Forecasting failed or skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire Answers\n",
    "\n",
    "**1. Defining the problem statements and where can this and modifications of this be used?**\n",
    "\n",
    "*   **Problem Statement:** Forecast the daily page views for various Wikipedia articles across different languages to predict and optimize ad placement effectiveness for AdEase clients.\n",
    "*   **Use Cases:**\n",
    "    *   **Ad Placement Optimization:** Identify pages/languages with high predicted future traffic for placing client ads.\n",
    "    *   **Budget Allocation:** Allocate advertising budgets based on expected views and potential ROI for different pages/languages.\n",
    "    *   **Campaign Planning:** Predict the impact of external events (like the provided campaign data) on page views to inform marketing strategies.\n",
    "    *   **Content Strategy:** Identify trending topics or languages based on view patterns.\n",
    "    *   **Resource Allocation (Wikipedia):** Could potentially be used by Wikimedia to anticipate server load.\n",
    "\n",
    "**2. Write 3 inferences you made from the data visualizations**\n",
    "\n",
    "    *(Note: These are expected inferences based on typical web traffic data; actual results depend on running the notebook)*\n",
    "    *   **Overall Trend:** There might be an overall increasing or decreasing trend in total page views over the 550 days, possibly with some fluctuations.\n",
    "    *   **Seasonality:** Clear weekly seasonality is expected (e.g., lower views on weekends) and potentially yearly seasonality (e.g., dips during major holidays).\n",
    "    *   **Language Dominance:** English ('en') pages likely dominate the total views, followed by other major languages like Japanese ('ja'), German ('de'), French ('fr'), etc., but the relative popularity might shift over time.\n",
    "    *   **(Bonus) Access Type:** Desktop views might be higher overall, but mobile access could show different growth trends.\n",
    "\n",
    "**3. What does the decomposition of series do?**\n",
    "\n",
    "*   Time series decomposition separates a time series into its underlying components, typically:\n",
    "    *   **Trend:** The long-term progression of the series (e.g., increasing, decreasing, or stable).\n",
    "    *   **Seasonality:** Patterns that repeat over a fixed period (e.g., daily, weekly, yearly cycles).\n",
    "    *   **Residual (or Remainder):** The irregular, random fluctuations left over after removing the trend and seasonality. It represents the noise in the series.\n",
    "*   Decomposition helps in understanding the different patterns driving the series' behavior and can be useful for modeling (e.g., modeling components separately or removing seasonality before applying certain models).\n",
    "\n",
    "**4. What level of differencing gave you a stationary series?**\n",
    "\n",
    "*   Based on the code structure and typical view count data, **first-order differencing (d=1)** is usually sufficient to achieve stationarity (i.e., stabilize the mean). The ADF test performed on `series_diff1` should confirm this with a p-value <= 0.05. If not, second-order differencing might be needed.\n",
    "\n",
    "**5. Difference between arima, sarima & sarimax.**\n",
    "\n",
    "*   **ARIMA (AutoRegressive Integrated Moving Average):** Models a time series based on its own past values (AR part), uses differencing to make the series stationary (I part), and incorporates errors made in past forecasts (MA part). It's defined by the order (p, d, q).\n",
    "*   **SARIMA (Seasonal ARIMA):** Extends ARIMA by adding components to model seasonality. It includes both non-seasonal parameters (p, d, q) and seasonal parameters (P, D, Q, s), where 's' is the length of the seasonal cycle (e.g., 7 for weekly, 12 for monthly).\n",
    "*   **SARIMAX (Seasonal ARIMA with eXogenous variables):** Further extends SARIMA by allowing the inclusion of external predictor variables (exogenous variables) that might influence the time series. The 'X' indicates the ability to include these external factors (like the campaign data in `exog_df`).\n",
    "\n",
    "**6. Compare the number of views in different languages**\n",
    "\n",
    "*   *(Based on expected results from visualization)* English ('en') pages are expected to have the highest total and average daily views by a significant margin.\n",
    "*   Other languages like Japanese ('ja'), German ('de'), French ('fr'), Spanish ('es'), Russian ('ru'), and Chinese ('zh') likely follow, but their relative ranking and view counts will vary.\n",
    "*   The visualization `Average Daily Views per Language (Top 7)` and the `Language Distribution` value counts provide specific insights into these comparisons after running the code.\n",
    "\n",
    "**7. What other methods other than grid search would be suitable to get the model for all languages?**\n",
    "\n",
    "*   **Auto ARIMA:** Libraries like `pmdarima` provide an `auto_arima` function that automatically searches for the optimal (S)ARIMA(X) order (p, d, q)(P, D, Q, s) based on information criteria like AIC or BIC. This is generally much more efficient than manual grid search.\n",
    "*   **Bayesian Optimization:** Techniques like Bayesian optimization can be used to search the parameter space more intelligently than a simple grid search, often finding good parameters faster.\n",
    "*   **Prophet's Automatic Tuning:** Prophet handles seasonality and trend fitting largely automatically, reducing the need for manual order selection compared to ARIMA/SARIMA.\n",
    "*   **Machine Learning Models:** For forecasting many series, tree-based models (like LightGBM, XGBoost) or deep learning models (like LSTMs, N-BEATS) trained on features derived from the time series (lags, rolling statistics, date features) can be effective, especially when combined with global modeling approaches (training one model across multiple related series).\n",
    "*   **Heuristics based on ACF/PACF:** While not fully automated, analyzing the ACF/PACF plots for each series (or clusters of similar series) can provide good starting points for model orders, reducing the search space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
